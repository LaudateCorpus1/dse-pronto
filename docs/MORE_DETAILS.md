# More Details

### Setup Notes

* **Can I install DSE artifacts without credentials?**  _Once upon a time_, DataStax Enterprise RPM artifacts weren't
  downloadable without a "DataStax Academy" login.  However, artifacts are now
  [publicly accessible](https://docs.datastax.com/en/install/6.7/install/installRHELdse.html), so the Packer AMI bake
  simply runs `yum install` after adding the appropriate repository.  No credentials needed.
* **Why do we need routes to a NATGW?**  Management scripts (built into the AMI by Packer) will need access to the AWS SDK in
  order to describe instances, locate EBS volumes, apply EC2 tags, etc.  Without these routes, instances in the Data subnet
  tier can't reach AWS HTTP endpoints like `ec2.amazonaws.com`.
* **Do I need to bake AMIs and deploy in the same account?**  No, but this repo currently assumes you will.  If you **aren't**
  baking and deploying AMIs in the same account, your images will need to be **shared** with the target deployment account.
  This can be done manually in the EC2 console, or automatically by Packer during the baking process.
  * To do it with Packer:  modify the [JSON file](../core-components/packer/cassandra/dse-cassandra-ami.json) in the packer
    dir, and share to other accounts with [ami_users](https://www.packer.io/docs/builders/amazon-ebs.html#ami_users), copy to
    other regions with [ami_regions](https://www.packer.io/docs/builders/amazon-ebs.html#ami_regions).
* **I see a "sudo pip install" in the Packer recipe.**  Yes, that's [gross](https://dev.to/elabftw/stop-using-sudo-pip-install-52mn).
  Packer installs everything as `ec2-user`, while cloud-init scripts are run as `root`.  So currently, we bypass any permission
  issues by explicitly installing pip packages for both users.  Fixing this is a TODO.
* **Can I create clusters in multiple VPCs?**  Yes, within the same account or across accounts; however, if you intend to
  connect two clusters together (for DR/replication) or monitor clusters across VPCs with the same OpsCenter, you'll require
  either [VPC peering](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html),
  a [Transit Gateway](https://docs.aws.amazon.com/vpc/latest/tgw/tgw-getting-started.html),
  or [PrivateLink](https://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html) to route between the VPC CIDRs.
* **Can I use this repo to deploy (non-DSE) Apache Cassandra?**  Not yet, but we plan to take a crack at that next.

### Packer Debugging

* If you have issues with the Packer AMI bake, you can pause Packer and SSH to the instance being provisioned:
  * Add the `-debug` option to the invocation of the `packer` executable at the bottom of [bake-ami.sh](../core-components/bake-ami.sh).
    This will force Packer to stop and wait for approval after each step, allowing you to SSH in before the instance is
    terminated.
  * Find your Packer instance's public IP, then SSH in with the following command:
    * `ssh -i $(find ./packer -name "ec2*.pem") ec2-user@${PUBLIC_IP_HERE}`

### Bootstrap Debugging

* After deployment with Terraform, during instance initialization, the [User Data](../core-components/terraform/modules/cassandra/templates.tf)
  for each instance will run a [bootstrap](../core-components/packer/cassandra/scripts/bootstrap.sh).  This script locates
  EBS volumes, reads EC2 tags, creates ENIs, swaps the active interface of the instance, and more.  If any stage of this
  bootstrap fails for some reason, you may wish to SSH in and examine the logs.
  * In order to SSH to one of your Cassandra nodes, you should be able to use the "ssh_config" file generated by `operations.sh`,
    however you'll need to make sure you're targeting the active IP address.
    * `ssh -F ./ssh_config <ip_address>`
    * If this times out, check the EC2 console.  The instance may have a secondary IP attached and marked as the active
      interface.  This is an ENI, created and managed separately from the instance.  This exists so each node can have a
      static IP across relaunches.
    * ![IP](./images/cassandra_ips.png)
  * To check the logs on the node, `sudo grep "cloud-init" /var/log/messages`
  * To check if bootstrap ran successfully, `cat /var/log/bootstrap_cassandra.log`
  * To retry any bootstrap actions, `cd /opt/dse/cassandra/scripts`
    * The main script can be run with `bootstrap.sh 1` (the "1" tells the script to start the DSE service)

### Restack IP Conflict (known issue)

Sometimes during restack operations, DSE will fail to start with the following exception:
```
ERROR [main] 2020-03-11 23:56:03,442  CassandraDaemon.java:829 - Exception encountered during startup
java.lang.RuntimeException: A node with address /${YOUR_IP} already exists, cancelling join. Use cassandra.replace_address if you want to replace this node.
        at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:561)
        at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:901)
```
The current theory (?) is that the node is terminated before `nodetool drain` completes flushing memtables to disk, causing
a mismatch in the "system" keyspace between the dead node's EBS volume and the rest of the cluster.

The resolution (workaround) is to add this option to your detailed JVM options and restart manually.
(See the notes on the `replace_address` option in [step 11 here](https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsReplaceNode.html).)
```
-Dcassandra.replace_address=${YOUR_IP}
```
**Do not** add this to your startup flags by default, however; it causes the DSE cluster to move data from replicas on other
nodes, a process which can take many hours (despite the fact that the data is already present on attached EBS volumes).

### Bootstrap Lock File (known issue)

* During bootstrap in _initial_ cluster deployment, there's one fairly crucial point where [the certs and keyfiles are created](../core-components/packer/cassandra/scripts/gen_server_keystores.sh).
  These must be shared across the whole cluster via s3, which means there's potential for a race condition:  all initializing
  instances could start creating key material simultaneously.
* The `gen_server_keystores.sh` script attempts to solve this by use of a "semaphore-like" locking system in s3.  Under
  normal circumstances, this means one instance will create a lock, generate keys, then release the lock allowing other
  instances to proceed, at which point they'll find the already-generated keys.
* If you see the following message repeating forever in `/var/log/bootstrap_cassandra.log`, there's a problem:
```
Found a lock file in s3://your-tfstate-bucket-name-here/your-cluster-name-here/files -- sleeping 5 seconds...
```
* This may mean one instance obtained a lock, then had some issue before it finished generating keys, so the lock was never
  released.
  * The lock file name will include the IP of the instance that created it.  SSH to that instance, and debug from there.
    Bootstrap and cloud-init logs should indicate what's going on.
  * If EC2 terminated and relaunched the locking instance, the best way to fix may just be to manually delete the lock file.
    This should allow one of the other nodes to lock, generate keys, and move on with life.
* **TODO:** improve or replace this lockfile concept with something more foolproof.  It doesn't typically fail, but when
  it does, it would be nice if the system could at least recover (e.g. a timeout while waiting for lock to clear; allow
  waiting nodes to forcibly delete a lock that doesn't seem to be going away).

### Unconnected Cluster

* Once the instance is up and running, and you've managed to SSH in, `nodetool` should indicate whether DSE is in an up-normal
  ("UN") state on each node:
```
[ec2-user@ip-10-0-0-104 ~]$ nodetool status
Datacenter: us-west-2
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens       Owns    Host ID                               Rack
UN  172.0.0.1   404.28 KiB  256          ?       95ba1239-94cc-4809-9535-f6f620f54122  us-west-2b
UN  172.0.0.2   403.35 KiB  256          ?       54f13ae9-bf55-485a-9dfb-663daa7b0a79  us-west-2a
UN  172.0.0.3   371.37 KiB  256          ?       1710316c-33f0-4a81-befa-7504a6e41592  us-west-2c

Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless
```
* If `nodetool status` doesn't show the whole cluster (e.g. each node is in a separate cluster, by itself) check
  `/var/log/cassandra/system.log` for errors.
  * If these logs contain an SSL error like `Certificate signature validation failed`, this may indicate a mismatch in
    the cert and keystore.  Note that this shouldn't happen if the bootstrap "lock file" (described above) is functioning
    properly.  But if it does happen:
    * Find the tfstate bucket, then go to the following paths and delete the certs and keystores for **all but one node**:
      * `s3://${TFSTATE_BUCKET}/${CLUSTER_NAME}/files/certs`
      * `s3://${TFSTATE_BUCKET}/${CLUSTER_NAME}/files/keystores`
    * Once this is done, either terminate/relaunch the other nodes entirely, or just SSH and run the bootstrap manually:
      * `sudo sh /opt/dse/cassandra/scripts/bootstrap.sh 1`
      * `sudo service dse restart`
    * This will pull the single remaining cert/keystore from the tfstate bucket, and now all nodes should share the same
      certs, and thus be able to connect.

### SSL Certs

If you have already deployed a cluster using terraform and would like to replace the SSL cert with your own, you will
need to upload your cert.

* Cassandra: 

Rename your certs to the file names in below code snippet.  **Ensure that your certs are named properly.**
```
aws s3 cp ./certs/ca-key s3://${BUCKET}/${account_name}/${vpc_name}/${cluster}/files/certs/ca-key --region ${region}
aws s3 cp ./certs/ca-cert s3://${BUCKET}/${account_name}/${vpc_name}/${cluster}/files/certs/ca-cert  --region ${region}
aws s3 cp ./keystores/server-truststore.jks s3://${BUCKET}/${account_name}/${vpc_name}/${cluster}/files/keystores/server-truststore.jks --region ${region}
```
You will need to run the restack cluster and re-register your cluster with opscenter:
```
./operations.sh -a [account name] -v [vpc name] -c [cluster name] -o restack
./operations.sh -a [account name] -v [vpc name] -c [cluster name] -o attach-to-opscenter
```

* Opscenter:

```
aws acm import-certificate --certificate file://opscenter.crt --private-key file://opscenter.key --region us-west-2
```
You will need to update `ssl_certificate_id ` in your configuration folder in
`[account name]/[vpc name]/opscenter-resources/opscenter.tfvars`, then restack opscenter by redeploying the module and
terminating the instance.